Setting up MAE with mask ratio 0.75 and patch size 16
Using device cuda
Setting up encoder with patch size 16, pe grid of 60 x 200
Setting up decoder with max lmx sequence length 1536, vocab file lmx_vocab.txt
Loaded pretrained mae state dict from mae_pre_train/pretrained_mae.pth
Setting up ViTOMR model
Model architecture
--------------------
ViTOMR(
  (encoder): OMREncoder(
    (projection): Linear(in_features=256, out_features=768, bias=True)
    (encoder_blocks): TransformerEncoder(
      (layers): ModuleList(
        (0-11): 12 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (linear1): Linear(in_features=768, out_features=3072, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=3072, out_features=768, bias=True)
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (decoder): OMRDecoder(
    (vocab_embedding): Embedding(227, 1024, padding_idx=1)
    (decoder_blocks): TransformerDecoder(
      (layers): ModuleList(
        (0-9): 10 x TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=4096, bias=True)
          (dropout): Dropout(p=0.15, inplace=False)
          (linear2): Linear(in_features=4096, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.15, inplace=False)
          (dropout2): Dropout(p=0.15, inplace=False)
          (dropout3): Dropout(p=0.15, inplace=False)
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (unembed): Linear(in_features=1024, out_features=227, bias=True)
  )
  (transition_head): Sequential(
    (0): Linear(in_features=768, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=4096, out_features=1024, bias=True)
  )
)
Trainable parameters count: 177351907
Setting up DataLoaders with batch size 32, shuffle, 26 workers, ragged collate function, pinned memory
Dataset augmentation probability: 0.3
Setting up AdamW with base learning rate 0.0002, betas (0.9, 0.95), weight decay 0.01
Setting up scheduler with 5 warm-up epochs, 100 total epochs, 1e-06 minimum learning rate, 1770 batches per epoch
Created directories omr_train, omr_train/checkpoints, omr_train/stats
OMR training for 100 epochs. Checkpointing every 10 epochs
Epoch 1
--------------------
Learning rate at epoch start: 0.00000100
Starting training
/home/ubuntu/.cache/pypoetry/virtualenvs/acai-omr-BQvANxl_-py3.12/lib/python3.12/site-packages/torch/nested/__init__.py:117: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)
  return torch._nested_tensor_from_tensor_list(ts, dtype, None, device, None)
[    32/ 56612]
[  3232/ 56612]
[  6432/ 56612]
[  9632/ 56612]
[ 12832/ 56612]
[ 16032/ 56612]
[ 19232/ 56612]
[ 22432/ 56612]
[ 25632/ 56612]
[ 28832/ 56612]
[ 32032/ 56612]
[ 35232/ 56612]
[ 38432/ 56612]
[ 41632/ 56612]
[ 44832/ 56612]
[ 48032/ 56612]
[ 51232/ 56612]
[ 54432/ 56612]
Average training loss over this epoch: 2.3782056830023643
Time for this training epoch: 520.88 seconds (8.68 minutes)
Starting validation
[   32/ 7499]
[ 1632/ 7499]
[ 3232/ 7499]
[ 4832/ 7499]
[ 6432/ 7499]
Average validation loss for this epoch: 1.7497450488678952
Epoch 2
--------------------
Learning rate at epoch start: 0.00004080
Starting training
[    32/ 56612]
[  3232/ 56612]
[  6432/ 56612]
[  9632/ 56612]
[ 12832/ 56612]
[ 16032/ 56612]
[ 19232/ 56612]
[ 22432/ 56612]
[ 25632/ 56612]
[ 28832/ 56612]
[ 32032/ 56612]
[ 35232/ 56612]
[ 38432/ 56612]
[ 41632/ 56612]
[ 44832/ 56612]
[ 48032/ 56612]
[ 51232/ 56612]
[ 54432/ 56612]
Average training loss over this epoch: 1.5964401825673162
Time for this training epoch: 524.55 seconds (8.74 minutes)
Starting validation
[   32/ 7499]
[ 1632/ 7499]
[ 3232/ 7499]
[ 4832/ 7499]
[ 6432/ 7499]
Average validation loss for this epoch: 1.4328730415790638
Epoch 3
--------------------
Learning rate at epoch start: 0.00008060
Starting training
[    32/ 56612]
[  3232/ 56612]
[  6432/ 56612]
[  9632/ 56612]
[ 12832/ 56612]
[ 16032/ 56612]
[ 19232/ 56612]
[ 22432/ 56612]
[ 25632/ 56612]
[ 28832/ 56612]
[ 32032/ 56612]
[ 35232/ 56612]
[ 38432/ 56612]
[ 41632/ 56612]
[ 44832/ 56612]
[ 48032/ 56612]
[ 51232/ 56612]
[ 54432/ 56612]
Average training loss over this epoch: 1.3474162972579569
Time for this training epoch: 520.61 seconds (8.68 minutes)
Starting validation
[   32/ 7499]
[ 1632/ 7499]
[ 3232/ 7499]
[ 4832/ 7499]
[ 6432/ 7499]
Average validation loss for this epoch: 1.2575485193983038
Epoch 4
--------------------
Learning rate at epoch start: 0.00012040
Starting training
[    32/ 56612]
[  3232/ 56612]
[  6432/ 56612]
[  9632/ 56612]
[ 12832/ 56612]
[ 16032/ 56612]
[ 19232/ 56612]
[ 22432/ 56612]
[ 25632/ 56612]
[ 28832/ 56612]
[ 32032/ 56612]
[ 35232/ 56612]
[ 38432/ 56612]
[ 41632/ 56612]
[ 44832/ 56612]
[ 48032/ 56612]
[ 51232/ 56612]
[ 54432/ 56612]
Average training loss over this epoch: 1.2030138881193042
Time for this training epoch: 521.13 seconds (8.69 minutes)
Starting validation
[   32/ 7499]
[ 1632/ 7499]
[ 3232/ 7499]
[ 4832/ 7499]
[ 6432/ 7499]
Average validation loss for this epoch: 1.1794040639349754
Epoch 5
--------------------
Learning rate at epoch start: 0.00016020
Starting training
[    32/ 56612]
[  3232/ 56612]
[  6432/ 56612]
[  9632/ 56612]
[ 12832/ 56612]
[ 16032/ 56612]
[ 19232/ 56612]
[ 22432/ 56612]
[ 25632/ 56612]
[ 28832/ 56612]
[ 32032/ 56612]
[ 35232/ 56612]
[ 38432/ 56612]
[ 41632/ 56612]
[ 44832/ 56612]
[ 48032/ 56612]
[ 51232/ 56612]
[ 54432/ 56612]
/home/ubuntu/.cache/pypoetry/virtualenvs/acai-omr-BQvANxl_-py3.12/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Average training loss over this epoch: 1.1138673558073529
Time for this training epoch: 523.15 seconds (8.72 minutes)
Starting validation
[   32/ 7499]
[ 1632/ 7499]
[ 3232/ 7499]
[ 4832/ 7499]
[ 6432/ 7499]
Average validation loss for this epoch: 1.0973122053958
Epoch 6
--------------------
Learning rate at epoch start: 0.00020000
Starting training
[    32/ 56612]
[  3232/ 56612]
[  6432/ 56612]
[  9632/ 56612]
[ 12832/ 56612]
[ 16032/ 56612]
[ 19232/ 56612]
[ 22432/ 56612]
[ 25632/ 56612]
[ 28832/ 56612]
[ 32032/ 56612]
[ 35232/ 56612]
[ 38432/ 56612]
[ 41632/ 56612]
[ 44832/ 56612]
[ 48032/ 56612]
[ 51232/ 56612]
[ 54432/ 56612]
Average training loss over this epoch: 1.0606895382121457
Time for this training epoch: 523.24 seconds (8.72 minutes)
Starting validation
[   32/ 7499]
[ 1632/ 7499]
[ 3232/ 7499]
[ 4832/ 7499]
[ 6432/ 7499]
Average validation loss for this epoch: 1.0501112691899563
Epoch 7
--------------------
Learning rate at epoch start: 0.00019995
Starting training
[    32/ 56612]
[  3232/ 56612]
[  6432/ 56612]
[  9632/ 56612]
[ 12832/ 56612]
[ 16032/ 56612]
[ 19232/ 56612]
[ 22432/ 56612]
[ 25632/ 56612]
[ 28832/ 56612]
[ 32032/ 56612]
[ 35232/ 56612]
[ 38432/ 56612]
[ 41632/ 56612]
[ 44832/ 56612]
[ 48032/ 56612]
[ 51232/ 56612]
[ 54432/ 56612]
Average training loss over this epoch: 1.0231457725756585
Time for this training epoch: 523.32 seconds (8.72 minutes)
Starting validation
[   32/ 7499]
[ 1632/ 7499]
[ 3232/ 7499]
[ 4832/ 7499]
[ 6432/ 7499]
Average validation loss for this epoch: 1.0345238515671262
Epoch 8
--------------------
Learning rate at epoch start: 0.00019978
Starting training
[    32/ 56612]
[  3232/ 56612]
[  6432/ 56612]
[  9632/ 56612]
[ 12832/ 56612]
[ 16032/ 56612]
[ 19232/ 56612]
[ 22432/ 56612]
[ 25632/ 56612]
[ 28832/ 56612]
[ 32032/ 56612]
[ 35232/ 56612]
[ 38432/ 56612]
[ 41632/ 56612]
[ 44832/ 56612]
[ 48032/ 56612]
[ 51232/ 56612]
[ 54432/ 56612]
Average training loss over this epoch: 1.0003004811577878
Time for this training epoch: 521.83 seconds (8.70 minutes)
Starting validation
[   32/ 7499]
[ 1632/ 7499]
[ 3232/ 7499]
[ 4832/ 7499]
[ 6432/ 7499]
Average validation loss for this epoch: 1.019726380896061
Epoch 9
--------------------
Learning rate at epoch start: 0.00019951
Starting training
[    32/ 56612]
[  3232/ 56612]
[  6432/ 56612]
[  9632/ 56612]
[ 12832/ 56612]
[ 16032/ 56612]
[ 19232/ 56612]
[ 22432/ 56612]
[ 25632/ 56612]
[ 28832/ 56612]
[ 32032/ 56612]
[ 35232/ 56612]
[ 38432/ 56612]
[ 41632/ 56612]
[ 44832/ 56612]
[ 48032/ 56612]
[ 51232/ 56612]
[ 54432/ 56612]
Average training loss over this epoch: 0.9847825615082757
Time for this training epoch: 524.56 seconds (8.74 minutes)
Starting validation
[   32/ 7499]
[ 1632/ 7499]
[ 3232/ 7499]
[ 4832/ 7499]
[ 6432/ 7499]
Average validation loss for this epoch: 1.0177008425935785
Epoch 10
--------------------
Learning rate at epoch start: 0.00019913
Starting training
[    32/ 56612]
[  3232/ 56612]
[  6432/ 56612]
[  9632/ 56612]
[ 12832/ 56612]
[ 16032/ 56612]
[ 19232/ 56612]
[ 22432/ 56612]
[ 25632/ 56612]
[ 28832/ 56612]
[ 32032/ 56612]
[ 35232/ 56612]
[ 38432/ 56612]
[ 41632/ 56612]
[ 44832/ 56612]
[ 48032/ 56612]
[ 51232/ 56612]
[ 54432/ 56612]
Average training loss over this epoch: 0.9721912624809028
Time for this training epoch: 522.46 seconds (8.71 minutes)
Starting validation
[   32/ 7499]
[ 1632/ 7499]
[ 3232/ 7499]
[ 4832/ 7499]
[ 6432/ 7499]
Average validation loss for this epoch: 1.0086990693782238
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to omr_train/checkpoints/epoch_10_checkpoint.pth
Checkpointing stats plots
Saving plot to omr_train/stats/losses.png
Saving plot to omr_train/stats/lrs.png
Writing training stats csv to omr_train/stats/training_stats.csv
Epoch 11
--------------------
Learning rate at epoch start: 0.00019864
Starting training
[    32/ 56612]
[  3232/ 56612]
[  6432/ 56612]
[  9632/ 56612]
[ 12832/ 56612]
[ 16032/ 56612]
[ 19232/ 56612]
[ 22432/ 56612]
[ 25632/ 56612]
[ 28832/ 56612]
[ 32032/ 56612]
[ 35232/ 56612]
[ 38432/ 56612]
[ 41632/ 56612]
[ 44832/ 56612]
[ 48032/ 56612]
[ 51232/ 56612]
[ 54432/ 56612]
Average training loss over this epoch: 0.9626368629056855
Time for this training epoch: 521.73 seconds (8.70 minutes)
Starting validation
[   32/ 7499]
[ 1632/ 7499]
[ 3232/ 7499]
[ 4832/ 7499]
[ 6432/ 7499]
Average validation loss for this epoch: 0.9967924125651095
Epoch 12
--------------------
Learning rate at epoch start: 0.00019805
Starting training
[    32/ 56612]
[  3232/ 56612]
[  6432/ 56612]
[  9632/ 56612]
