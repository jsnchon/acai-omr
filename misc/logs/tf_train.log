Using device cuda
Setting up encoder with patch size 16, pe grid of 60 x 200, fine-tuning last 12 layers
Setting up decoder with max lmx sequence length 1536, vocab file lmx_vocab.txt
Loaded pretrained mae state dict from mae_pre_train/pretrained_mae.pth
Setting up ViTOMR model

Created directories tf_omr_train, tf_omr_train/checkpoints

Model architecture
--------------------------------------------------
ScheduledSamplingViTOMR(
  (encoder): FineTuneOMREncoder(
    (unfold): Unfold(kernel_size=16, dilation=1, padding=0, stride=16)
    (projection): Linear(in_features=256, out_features=768, bias=True)
    (fine_tune_blocks): TransformerEncoder(
      (layers): ModuleList(
        (0-11): 12 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (linear1): Linear(in_features=768, out_features=3072, bias=True)
          (dropout): Dropout(p=0.05, inplace=False)
          (linear2): Linear(in_features=3072, out_features=768, bias=True)
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.05, inplace=False)
          (dropout2): Dropout(p=0.05, inplace=False)
        )
      )
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (transition_head): Sequential(
    (0): Linear(in_features=768, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.05, inplace=False)
    (3): Linear(in_features=4096, out_features=1024, bias=True)
  )
  (decoder): OMRDecoder(
    (vocab_embedding): Embedding(227, 1024, padding_idx=1)
    (decoder_blocks): TransformerDecoder(
      (layers): ModuleList(
        (0-11): 12 x TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=4096, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (unembed): Linear(in_features=1024, out_features=227, bias=True)
  )
)
Trainable parameters count
Encoder: 94469376
Transition head: 7345152
Decoder: 203600099
Total: 305414627

General hyperparameters
--------------------------------------------------
Epochs: 40
Warmup epochs: 2
Checkpoint frequency: 10 epochs
Base lr: 0.0001
Fine tune base lr: 1e-05, layer-wise decay factor of 0.9
Minimum lr: 1e-06
AdamW betas: (0.9, 0.95), weight decay: 0.01
Batch size: 8
Gradient accumulation steps: 8
Number of DataLoader workers: 26

Encoder fine-tune base lrs by layer: [1e-05, 9e-06, 8.1e-06, 7.290000000000001e-06, 6.561e-06, 5.904900000000001e-06, 5.314410000000001e-06, 4.782969000000001e-06, 4.304672100000001e-06, 3.8742048900000015e-06, 3.486784401000001e-06, 3.138105960900001e-06]

Regularization hyperparameters
--------------------------------------------------
Image augmentation probability: 0.5
Encoder dropout: 0.05
Transition head dropout: 0.05
Decoder dropout: 0.1
Label smoothing: 0.0

Teacher forcing hyperparameters
--------------------------------------------------
Initial teacher forcing per-token probability: 1.0
Minimum teacher forcing probability: 0.0
Initial Gumbel-Softmax tau: 5.0
Minimum Gumbel-Softmax tau: 0.1
Number of epochs with soft prediction sampling: 20
Annealing teacher forcing probability and tau over 35 epochs
OMR training for 40 epochs. Checkpointing every 10 epochs

Epoch 1
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00000050
Fine-tune learning rate: 0.00000005
Teacher forcing probability: 1.00000000
Gumbel-softmax tau: 5.00000000
Using hard sampling: False
Starting training
/home/ubuntu/.cache/pypoetry/virtualenvs/acai-omr-BQvANxl_-py3.12/lib/python3.12/site-packages/torch/nested/__init__.py:117: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)
  return torch._nested_tensor_from_tensor_list(ts, dtype, None, device, None)
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 1.9684271487001141
Time for this training epoch: 1692.53 seconds (28.21 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 1.063625283841131

Epoch 2
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00005025
Fine-tune learning rate: 0.00000503
Teacher forcing probability: 0.97146086
Gumbel-softmax tau: 4.47180497
Using hard sampling: False
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
/home/ubuntu/.cache/pypoetry/virtualenvs/acai-omr-BQvANxl_-py3.12/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Average training loss over this epoch: 0.9225240883827883
Time for this training epoch: 1691.34 seconds (28.19 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.6954580977527317

Epoch 3
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00010000
Fine-tune learning rate: 0.00001000
Teacher forcing probability: 0.94288943
Gumbel-softmax tau: 3.99890285
Using hard sampling: False
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.6439918605885769
Time for this training epoch: 1684.92 seconds (28.08 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.4611702660507739

Epoch 4
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00009983
Fine-tune learning rate: 0.00000998
Teacher forcing probability: 0.91431800
Gumbel-softmax tau: 3.57601106
Using hard sampling: False
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.48205853618183464
Time for this training epoch: 1716.72 seconds (28.61 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.3505832388171001

Epoch 5
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00009932
Fine-tune learning rate: 0.00000994
Teacher forcing probability: 0.88574657
Gumbel-softmax tau: 3.19784091
Using hard sampling: False
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.38682274115494925
Time for this training epoch: 1741.27 seconds (29.02 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.2818522532103158

Epoch 6
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00009849
Fine-tune learning rate: 0.00000986
Teacher forcing probability: 0.85717514
Gumbel-softmax tau: 2.85966298
Using hard sampling: False
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.3179292442587383
Time for this training epoch: 1675.87 seconds (27.93 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.21701015964912962

Epoch 7
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00009732
Fine-tune learning rate: 0.00000976
Teacher forcing probability: 0.82860371
Gumbel-softmax tau: 2.55724803
Using hard sampling: False
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.2691460139728357
Time for this training epoch: 1677.64 seconds (27.96 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.19838573319761993

Epoch 8
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00009583
Fine-tune learning rate: 0.00000962
Teacher forcing probability: 0.80003228
Gumbel-softmax tau: 2.28681406
Using hard sampling: False
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.23431338687156644
Time for this training epoch: 1675.81 seconds (27.93 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.1795111939882927

Epoch 9
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00009403
Fine-tune learning rate: 0.00000946
Teacher forcing probability: 0.77146086
Gumbel-softmax tau: 2.04497900
Using hard sampling: False
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.20737054704634403
Time for this training epoch: 1664.43 seconds (27.74 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.16725880928289916

Epoch 10
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00009194
Fine-tune learning rate: 0.00000927
Teacher forcing probability: 0.74288943
Gumbel-softmax tau: 1.82871848
Using hard sampling: False
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.1877740144522215
Time for this training epoch: 1671.68 seconds (27.86 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.13910309972761792
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to tf_omr_train/checkpoints/epoch_10_checkpoint.pth
Saving training stats csv

Epoch 11
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00008956
Fine-tune learning rate: 0.00000905
Teacher forcing probability: 0.71431800
Gumbel-softmax tau: 1.63532792
Using hard sampling: False
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.17136833144930083
Time for this training epoch: 1665.67 seconds (27.76 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.13910459221275187

Epoch 12
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00008692
Fine-tune learning rate: 0.00000881
Teacher forcing probability: 0.68574657
Gumbel-softmax tau: 1.46238880
Using hard sampling: False
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.15904201666714107
Time for this training epoch: 1676.24 seconds (27.94 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.13911265060543887

Epoch 13
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00008403
Fine-tune learning rate: 0.00000855
Teacher forcing probability: 0.65717514
Gumbel-softmax tau: 1.30773833
Using hard sampling: False
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.14897491684994388
Time for this training epoch: 1670.49 seconds (27.84 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.1261105355429354

Epoch 14
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00008090
Fine-tune learning rate: 0.00000826
Teacher forcing probability: 0.62860371
Gumbel-softmax tau: 1.16944244
Using hard sampling: False
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.1385847544185717
Time for this training epoch: 1671.91 seconds (27.87 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.12811458282726707

Epoch 15
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00007757
Fine-tune learning rate: 0.00000796
Teacher forcing probability: 0.60003228
Gumbel-softmax tau: 1.04577162
Using hard sampling: False
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.1295164756066818
Time for this training epoch: 1672.07 seconds (27.87 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.14172116125316253

Epoch 16
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00007406
Fine-tune learning rate: 0.00000764
Teacher forcing probability: 0.57146086
Gumbel-softmax tau: 0.93517923
Using hard sampling: False
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.1254325200909778
Time for this training epoch: 1679.60 seconds (27.99 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.11374031067657064

Epoch 17
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00007038
Fine-tune learning rate: 0.00000731
Teacher forcing probability: 0.54288943
Gumbel-softmax tau: 0.83628220
Using hard sampling: False
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.12065236037682599
Time for this training epoch: 1750.12 seconds (29.17 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.11286369719277464

Epoch 18
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00006657
Fine-tune learning rate: 0.00000696
Teacher forcing probability: 0.51431800
Gumbel-softmax tau: 0.74784372
Using hard sampling: False
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.11802245067637553
Time for this training epoch: 1693.03 seconds (28.22 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.10846657026856979

Epoch 19
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00006265
Fine-tune learning rate: 0.00000660
Teacher forcing probability: 0.48574657
Gumbel-softmax tau: 0.66875779
Using hard sampling: False
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.11559722856212483
Time for this training epoch: 1674.72 seconds (27.91 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.11149028926960695

Epoch 20
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00005865
Fine-tune learning rate: 0.00000624
Teacher forcing probability: 0.45717514
Gumbel-softmax tau: 0.59803534
Using hard sampling: False
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.1152589053325634
Time for this training epoch: 1674.36 seconds (27.91 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.11826718137063372
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to tf_omr_train/checkpoints/epoch_20_checkpoint.pth
Saving training stats csv

Epoch 21
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00005459
Fine-tune learning rate: 0.00000587
Teacher forcing probability: 0.42860371
Gumbel-softmax tau: 0.53479194
Using hard sampling: False
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.14795322889919812
Time for this training epoch: 1678.12 seconds (27.97 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.06379948056805362

Epoch 22
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00005050
Fine-tune learning rate: 0.00000550
Teacher forcing probability: 0.40003228
Gumbel-softmax tau: 0.47823665
Using hard sampling: True
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.12746959829775878
Time for this training epoch: 1683.15 seconds (28.05 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.059515822824261495

Epoch 23
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00004641
Fine-tune learning rate: 0.00000513
Teacher forcing probability: 0.37146086
Gumbel-softmax tau: 0.42766219
Using hard sampling: True
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.11619557613253981
Time for this training epoch: 1675.43 seconds (27.92 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.05835074555527951

Epoch 24
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00004235
Fine-tune learning rate: 0.00000476
Teacher forcing probability: 0.34288943
Gumbel-softmax tau: 0.38243607
Using hard sampling: True
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.10679999881314652
Time for this training epoch: 1686.24 seconds (28.10 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.05794512748773863

Epoch 25
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00003835
Fine-tune learning rate: 0.00000440
Teacher forcing probability: 0.31431800
Gumbel-softmax tau: 0.34199271
Using hard sampling: True
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.09988787941000614
Time for this training epoch: 1689.72 seconds (28.16 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.05622298192187671

Epoch 26
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00003443
Fine-tune learning rate: 0.00000404
Teacher forcing probability: 0.28574657
Gumbel-softmax tau: 0.30582631
Using hard sampling: True
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.09288304720389248
Time for this training epoch: 1686.20 seconds (28.10 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.055930692280247024

Epoch 27
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00003062
Fine-tune learning rate: 0.00000369
Teacher forcing probability: 0.25717514
Gumbel-softmax tau: 0.27348458
Using hard sampling: True
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.0881095311498635
Time for this training epoch: 1689.94 seconds (28.17 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.05700278395800782

Epoch 28
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00002694
Fine-tune learning rate: 0.00000336
Teacher forcing probability: 0.22860371
Gumbel-softmax tau: 0.24456305
Using hard sampling: True
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.08341944932064334
Time for this training epoch: 1690.32 seconds (28.17 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.05484071350060721

Epoch 29
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00002343
Fine-tune learning rate: 0.00000304
Teacher forcing probability: 0.20003228
Gumbel-softmax tau: 0.21870003
Using hard sampling: True
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.07921432008008372
Time for this training epoch: 1687.80 seconds (28.13 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.05452114039014822

Epoch 30
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00002010
Fine-tune learning rate: 0.00000274
Teacher forcing probability: 0.17146086
Gumbel-softmax tau: 0.19557208
Using hard sampling: True
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.07557030559928161
Time for this training epoch: 1707.32 seconds (28.46 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.0537422511169115
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to tf_omr_train/checkpoints/epoch_30_checkpoint.pth
Saving training stats csv

Epoch 31
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00001697
Fine-tune learning rate: 0.00000245
Teacher forcing probability: 0.14288943
Gumbel-softmax tau: 0.17488995
Using hard sampling: True
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.07234873921996393
Time for this training epoch: 1696.87 seconds (28.28 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.054345187096139676

Epoch 32
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00001408
Fine-tune learning rate: 0.00000219
Teacher forcing probability: 0.11431800
Gumbel-softmax tau: 0.15639499
Using hard sampling: True
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.06961983118324175
Time for this training epoch: 1691.53 seconds (28.19 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.053030816655455115

Epoch 33
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00001144
Fine-tune learning rate: 0.00000195
Teacher forcing probability: 0.08574657
Gumbel-softmax tau: 0.13985592
Using hard sampling: True
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.06792619756231592
Time for this training epoch: 1681.09 seconds (28.02 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.05311974675821782

Epoch 34
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00000906
Fine-tune learning rate: 0.00000173
Teacher forcing probability: 0.05717514
Gumbel-softmax tau: 0.12506588
Using hard sampling: True
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.06592614149045206
Time for this training epoch: 1683.10 seconds (28.05 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.05324111344714238

Epoch 35
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00000697
Fine-tune learning rate: 0.00000154
Teacher forcing probability: 0.02860371
Gumbel-softmax tau: 0.11183992
Using hard sampling: True
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.06480641033856012
Time for this training epoch: 1689.40 seconds (28.16 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.05182096478274849

Epoch 36
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00000517
Fine-tune learning rate: 0.00000138
Teacher forcing probability: 0.00003228
Gumbel-softmax tau: 0.10001263
Using hard sampling: True
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.06341108451566467
Time for this training epoch: 1690.05 seconds (28.17 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.053320923910428114

Epoch 37
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00000368
Fine-tune learning rate: 0.00000124
Teacher forcing probability: 0.00000000
Gumbel-softmax tau: 0.10000000
Using hard sampling: True
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.06119043957018956
Time for this training epoch: 1685.31 seconds (28.09 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.05198675813699471

Epoch 38
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00000251
Fine-tune learning rate: 0.00000114
Teacher forcing probability: 0.00000000
Gumbel-softmax tau: 0.10000000
Using hard sampling: True
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.060113794141401264
Time for this training epoch: 1682.98 seconds (28.05 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.05228355563387696

Epoch 39
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00000168
Fine-tune learning rate: 0.00000106
Teacher forcing probability: 0.00000000
Gumbel-softmax tau: 0.10000000
Using hard sampling: True
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.05837241506388707
Time for this training epoch: 1686.05 seconds (28.10 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.0520718783334539

Epoch 40
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00000117
Fine-tune learning rate: 0.00000102
Teacher forcing probability: 0.00000000
Gumbel-softmax tau: 0.10000000
Using hard sampling: True
Starting training
[    8/56612]
[  808/56612]
[ 1608/56612]
[ 2408/56612]
[ 3208/56612]
[ 4008/56612]
[ 4808/56612]
[ 5608/56612]
[ 6408/56612]
[ 7208/56612]
[ 8008/56612]
[ 8808/56612]
[ 9608/56612]
[10408/56612]
[11208/56612]
[12008/56612]
[12808/56612]
[13608/56612]
[14408/56612]
[15208/56612]
[16008/56612]
[16808/56612]
[17608/56612]
[18408/56612]
[19208/56612]
[20008/56612]
[20808/56612]
[21608/56612]
[22408/56612]
[23208/56612]
[24008/56612]
[24808/56612]
[25608/56612]
[26408/56612]
[27208/56612]
[28008/56612]
[28808/56612]
[29608/56612]
[30408/56612]
[31208/56612]
[32008/56612]
[32808/56612]
[33608/56612]
[34408/56612]
[35208/56612]
[36008/56612]
[36808/56612]
[37608/56612]
[38408/56612]
[39208/56612]
[40008/56612]
[40808/56612]
[41608/56612]
[42408/56612]
[43208/56612]
[44008/56612]
[44808/56612]
[45608/56612]
[46408/56612]
[47208/56612]
[48008/56612]
[48808/56612]
[49608/56612]
[50408/56612]
[51208/56612]
[52008/56612]
[52808/56612]
[53608/56612]
[54408/56612]
[55208/56612]
[56008/56612]
Average training loss over this epoch: 0.057977949242993626
Time for this training epoch: 1682.71 seconds (28.05 minutes)
Starting validation
[    8/ 7499]
[  408/ 7499]
[  808/ 7499]
[ 1208/ 7499]
[ 1608/ 7499]
[ 2008/ 7499]
[ 2408/ 7499]
[ 2808/ 7499]
[ 3208/ 7499]
[ 3608/ 7499]
[ 4008/ 7499]
[ 4408/ 7499]
[ 4808/ 7499]
[ 5208/ 7499]
[ 5608/ 7499]
[ 6008/ 7499]
[ 6408/ 7499]
[ 6808/ 7499]
[ 7208/ 7499]
Average validation loss for this epoch: 0.05226046874149164
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to tf_omr_train/checkpoints/epoch_40_checkpoint.pth
Saving training stats csv
Saving final omr training state
Saving omr training state to tf_omr_train/ending_omr_train_state.pth
Saving final model state dict separately to tf_omr_train/vitomr.pth
