Using device cuda
Setting up encoder with patch size 16, pe grid of 60 x 200, fine-tuning last 12 layers
Setting up decoder with max lmx sequence length 1536, vocab file lmx_vocab.txt
Loaded pretrained mae state dict from mae_pre_train/pretrained_mae.pth
Setting up ViTOMR model

Created directories tf_omr_train, tf_omr_train/checkpoints

Model architecture
--------------------------------------------------
ScheduledSamplingViTOMR(
  (encoder): FineTuneOMREncoder(
    (unfold): Unfold(kernel_size=16, dilation=1, padding=0, stride=16)
    (projection): Linear(in_features=256, out_features=768, bias=True)
    (fine_tune_blocks): TransformerEncoder(
      (layers): ModuleList(
        (0-11): 12 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (linear1): Linear(in_features=768, out_features=3072, bias=True)
          (dropout): Dropout(p=0.05, inplace=False)
          (linear2): Linear(in_features=3072, out_features=768, bias=True)
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.05, inplace=False)
          (dropout2): Dropout(p=0.05, inplace=False)
        )
      )
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (transition_head): Sequential(
    (0): Linear(in_features=768, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.05, inplace=False)
    (3): Linear(in_features=4096, out_features=1024, bias=True)
  )
  (decoder): OMRDecoder(
    (vocab_embedding): Embedding(227, 1024, padding_idx=1)
    (decoder_blocks): TransformerDecoder(
      (layers): ModuleList(
        (0-11): 12 x TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=4096, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (unembed): Linear(in_features=1024, out_features=227, bias=True)
  )
)
Trainable parameters count
Encoder: 94469376
Transition head: 7345152
Decoder: 203600099
Total: 305414627

General hyperparameters
--------------------------------------------------
Epochs: 60
Warmup epochs: 3
Checkpoint frequency: 10 epochs
Base lr: 0.0001
Fine tune base lr: 1e-05, layer-wise decay factor of 0.9
Minimum lr: 1e-06
AdamW betas: (0.9, 0.95), weight decay: 0.01
Batch size: 16
Gradient accumulation steps: 4
Number of DataLoader workers: 26

Encoder fine-tune base lrs by layer: [1e-05, 9e-06, 8.1e-06, 7.290000000000001e-06, 6.561e-06, 5.904900000000001e-06, 5.314410000000001e-06, 4.782969000000001e-06, 4.304672100000001e-06, 3.8742048900000015e-06, 3.486784401000001e-06, 3.138105960900001e-06]

Regularization hyperparameters
--------------------------------------------------
Image augmentation probability: 0.4
Encoder dropout: 0.05
Transition head dropout: 0.05
Decoder dropout: 0.1
Label smoothing: 0.0

Teacher forcing hyperparameters
--------------------------------------------------
Initial teacher forcing per-token probability: 1.0
Minimum teacher forcing probability: 0.1
Initial Gumbel-Softmax tau: 5.0
Minimum Gumbel-Softmax tau: 0.1
Number of epochs with soft prediction sampling: 30
Annealing teacher forcing probability and tau over 50 epochs
OMR training for 60 epochs. Checkpointing every 10 epochs

Epoch 1
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00000050
Fine-tune learning rate: 0.00000005
Teacher forcing probability: 1.00000000
Gumbel-softmax tau: 5.00000000
Using hard sampling: False
Starting training
/home/ubuntu/.cache/pypoetry/virtualenvs/acai-omr-BQvANxl_-py3.12/lib/python3.12/site-packages/torch/nested/__init__.py:117: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)
  return torch._nested_tensor_from_tensor_list(ts, dtype, None, device, None)
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 2.1003652896945777
Time for this training epoch: 1220.06 seconds (20.33 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 1.1822569576153623

Epoch 2
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00003367
Fine-tune learning rate: 0.00000337
Teacher forcing probability: 0.98202034
Gumbel-softmax tau: 4.62411897
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.997159408273317
Time for this training epoch: 1220.93 seconds (20.35 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.7706214265782696

Epoch 3
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00006683
Fine-tune learning rate: 0.00000668
Teacher forcing probability: 0.96402034
Gumbel-softmax tau: 4.27611719
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
/home/ubuntu/.cache/pypoetry/virtualenvs/acai-omr-BQvANxl_-py3.12/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Average training loss over this epoch: 0.7018681090071431
Time for this training epoch: 1215.54 seconds (20.26 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.5172288102000507

Epoch 4
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00010000
Fine-tune learning rate: 0.00001000
Teacher forcing probability: 0.94602034
Gumbel-softmax tau: 3.95430532
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.5038259024083126
Time for this training epoch: 1218.79 seconds (20.31 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.3839705512086466

Epoch 5
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00009992
Fine-tune learning rate: 0.00000999
Teacher forcing probability: 0.92802034
Gumbel-softmax tau: 3.65671235
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.3853738797682765
Time for this training epoch: 1216.04 seconds (20.27 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.28261833086705157

Epoch 6
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00009970
Fine-tune learning rate: 0.00000997
Teacher forcing probability: 0.91002034
Gumbel-softmax tau: 3.38151563
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.31058498912610116
Time for this training epoch: 1217.98 seconds (20.30 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.22637342196156476

Epoch 7
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00009932
Fine-tune learning rate: 0.00000994
Teacher forcing probability: 0.89202034
Gumbel-softmax tau: 3.12702965
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.25902108901569015
Time for this training epoch: 1215.79 seconds (20.26 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.18184684272577514

Epoch 8
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00009880
Fine-tune learning rate: 0.00000989
Teacher forcing probability: 0.87402034
Gumbel-softmax tau: 2.89169576
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.22187477206654993
Time for this training epoch: 1218.45 seconds (20.31 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.16363714397080673

Epoch 9
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00009813
Fine-tune learning rate: 0.00000983
Teacher forcing probability: 0.85602034
Gumbel-softmax tau: 2.67407263
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.19636273909150695
Time for this training epoch: 1219.12 seconds (20.32 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.22113380330140148

Epoch 10
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00009732
Fine-tune learning rate: 0.00000976
Teacher forcing probability: 0.83802034
Gumbel-softmax tau: 2.47282737
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.17598653096083566
Time for this training epoch: 1222.72 seconds (20.38 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.13027321103253345
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to tf_omr_train/checkpoints/epoch_10_checkpoint.pth
Saving training stats csv

Epoch 11
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00009636
Fine-tune learning rate: 0.00000967
Teacher forcing probability: 0.82002034
Gumbel-softmax tau: 2.28672741
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.16019535893334882
Time for this training epoch: 1216.47 seconds (20.27 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.1262437280084787

Epoch 12
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00009527
Fine-tune learning rate: 0.00000957
Teacher forcing probability: 0.80202034
Gumbel-softmax tau: 2.11463297
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.1467587202240027
Time for this training epoch: 1220.00 seconds (20.33 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.13337105287036408

Epoch 13
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00009403
Fine-tune learning rate: 0.00000946
Teacher forcing probability: 0.78402034
Gumbel-softmax tau: 1.95548999
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.13657647404766043
Time for this training epoch: 1239.07 seconds (20.65 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.11496336752576615

Epoch 14
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00009267
Fine-tune learning rate: 0.00000933
Teacher forcing probability: 0.76602034
Gumbel-softmax tau: 1.80832380
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.12774708411596056
Time for this training epoch: 1239.06 seconds (20.65 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.11852286623389736

Epoch 15
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00009118
Fine-tune learning rate: 0.00000920
Teacher forcing probability: 0.74802034
Gumbel-softmax tau: 1.67223303
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.11922706579397412
Time for this training epoch: 1220.11 seconds (20.34 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.11311808745585271

Epoch 16
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00008956
Fine-tune learning rate: 0.00000905
Teacher forcing probability: 0.73002034
Gumbel-softmax tau: 1.54638418
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.1135999720925731
Time for this training epoch: 1221.27 seconds (20.35 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.11014349263773035

Epoch 17
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00008783
Fine-tune learning rate: 0.00000889
Teacher forcing probability: 0.71202034
Gumbel-softmax tau: 1.43000645
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.10795366289223039
Time for this training epoch: 1220.44 seconds (20.34 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.09690749624184072

Epoch 18
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00008598
Fine-tune learning rate: 0.00000873
Teacher forcing probability: 0.69402034
Gumbel-softmax tau: 1.32238708
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.1031189826123709
Time for this training epoch: 1218.47 seconds (20.31 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.09490795900969744

Epoch 19
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00008403
Fine-tune learning rate: 0.00000855
Teacher forcing probability: 0.67602034
Gumbel-softmax tau: 1.22286692
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.09951964483364015
Time for this training epoch: 1219.28 seconds (20.32 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.11226473793165008

Epoch 20
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00008197
Fine-tune learning rate: 0.00000836
Teacher forcing probability: 0.65802034
Gumbel-softmax tau: 1.13083645
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.09697269459733202
Time for this training epoch: 1220.75 seconds (20.35 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.09908563092049123
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to tf_omr_train/checkpoints/epoch_20_checkpoint.pth
Saving training stats csv

Epoch 21
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00007982
Fine-tune learning rate: 0.00000817
Teacher forcing probability: 0.64002034
Gumbel-softmax tau: 1.04573200
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.09374717246992637
Time for this training epoch: 1215.98 seconds (20.27 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.09273975542676982

Epoch 22
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00007757
Fine-tune learning rate: 0.00000796
Teacher forcing probability: 0.62202034
Gumbel-softmax tau: 0.96703234
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.09155108356402566
Time for this training epoch: 1216.20 seconds (20.27 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.08402322811374405

Epoch 23
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00007525
Fine-tune learning rate: 0.00000775
Teacher forcing probability: 0.60402034
Gumbel-softmax tau: 0.89425545
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.09027160728504494
Time for this training epoch: 1222.65 seconds (20.38 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.08841195685078086

Epoch 24
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00007285
Fine-tune learning rate: 0.00000753
Teacher forcing probability: 0.58602034
Gumbel-softmax tau: 0.82695561
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.08939367968470119
Time for this training epoch: 1219.19 seconds (20.32 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.09413743623172932

Epoch 25
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00007038
Fine-tune learning rate: 0.00000731
Teacher forcing probability: 0.56802034
Gumbel-softmax tau: 0.76472061
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.08806586912410783
Time for this training epoch: 1221.39 seconds (20.36 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.09028984750035221

Epoch 26
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00006786
Fine-tune learning rate: 0.00000708
Teacher forcing probability: 0.55002034
Gumbel-softmax tau: 0.70716930
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.08889199239913691
Time for this training epoch: 1224.63 seconds (20.41 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.08706603687939676

Epoch 27
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00006528
Fine-tune learning rate: 0.00000684
Teacher forcing probability: 0.53202034
Gumbel-softmax tau: 0.65394918
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.08884266505239458
Time for this training epoch: 1239.89 seconds (20.66 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.07877376174796492

Epoch 28
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00006265
Fine-tune learning rate: 0.00000660
Teacher forcing probability: 0.51402034
Gumbel-softmax tau: 0.60473429
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.08838622569174677
Time for this training epoch: 1252.56 seconds (20.88 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.07759809376659996

Epoch 29
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00005999
Fine-tune learning rate: 0.00000636
Teacher forcing probability: 0.49602034
Gumbel-softmax tau: 0.55922322
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.0882230150979407
Time for this training epoch: 1254.19 seconds (20.90 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.08534169441331298

Epoch 30
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00005730
Fine-tune learning rate: 0.00000612
Teacher forcing probability: 0.47802034
Gumbel-softmax tau: 0.51713722
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.08877559778482925
Time for this training epoch: 1227.42 seconds (20.46 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.08109136982989718
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to tf_omr_train/checkpoints/epoch_30_checkpoint.pth
Saving training stats csv

Epoch 31
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00005459
Fine-tune learning rate: 0.00000587
Teacher forcing probability: 0.46002034
Gumbel-softmax tau: 0.47821853
Using hard sampling: False
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.11031188399723843
Time for this training epoch: 1222.89 seconds (20.38 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.06100015358518817

Epoch 32
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00005186
Fine-tune learning rate: 0.00000562
Teacher forcing probability: 0.44202034
Gumbel-softmax tau: 0.44222877
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.09266690035277199
Time for this training epoch: 1225.61 seconds (20.43 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.05856618963317004

Epoch 33
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00004914
Fine-tune learning rate: 0.00000538
Teacher forcing probability: 0.42402034
Gumbel-softmax tau: 0.40894754
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.08487758735222697
Time for this training epoch: 1222.37 seconds (20.37 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.05785160398146491

Epoch 34
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00004641
Fine-tune learning rate: 0.00000513
Teacher forcing probability: 0.40602034
Gumbel-softmax tau: 0.37817098
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.07884988066795334
Time for this training epoch: 1220.24 seconds (20.34 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.0565589907428206

Epoch 35
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00004370
Fine-tune learning rate: 0.00000488
Teacher forcing probability: 0.38802034
Gumbel-softmax tau: 0.34971060
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.07410556640908623
Time for this training epoch: 1220.73 seconds (20.35 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.05664430448471674

Epoch 36
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00004101
Fine-tune learning rate: 0.00000464
Teacher forcing probability: 0.37002034
Gumbel-softmax tau: 0.32339209
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.06994383758332398
Time for this training epoch: 1219.98 seconds (20.33 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.0560013109833987

Epoch 37
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00003835
Fine-tune learning rate: 0.00000440
Teacher forcing probability: 0.35202034
Gumbel-softmax tau: 0.29905426
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.06622293665394603
Time for this training epoch: 1223.07 seconds (20.38 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.055302111058632954

Epoch 38
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00003572
Fine-tune learning rate: 0.00000416
Teacher forcing probability: 0.33402034
Gumbel-softmax tau: 0.27654805
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.06253149546919679
Time for this training epoch: 1223.32 seconds (20.39 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.05596532906963627

Epoch 39
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00003314
Fine-tune learning rate: 0.00000392
Teacher forcing probability: 0.31602034
Gumbel-softmax tau: 0.25573560
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.05912906996033166
Time for this training epoch: 1229.18 seconds (20.49 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.05668362866300764

Epoch 40
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00003062
Fine-tune learning rate: 0.00000369
Teacher forcing probability: 0.29802034
Gumbel-softmax tau: 0.23648946
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.05582233274189214
Time for this training epoch: 1224.77 seconds (20.41 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.0568245798905394
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to tf_omr_train/checkpoints/epoch_40_checkpoint.pth
Saving training stats csv

Epoch 41
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00002815
Fine-tune learning rate: 0.00000347
Teacher forcing probability: 0.28002034
Gumbel-softmax tau: 0.21869175
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.053915385525425207
Time for this training epoch: 1237.96 seconds (20.63 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.055181303478555

Epoch 42
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00002575
Fine-tune learning rate: 0.00000325
Teacher forcing probability: 0.26202034
Gumbel-softmax tau: 0.20223345
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.05144838293433804
Time for this training epoch: 1221.51 seconds (20.36 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.056382447074074096

Epoch 43
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00002343
Fine-tune learning rate: 0.00000304
Teacher forcing probability: 0.24402034
Gumbel-softmax tau: 0.18701377
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.049197207435657114
Time for this training epoch: 1226.11 seconds (20.44 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.05475007463842313

Epoch 44
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00002118
Fine-tune learning rate: 0.00000283
Teacher forcing probability: 0.22602034
Gumbel-softmax tau: 0.17293950
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.04706509522743698
Time for this training epoch: 1223.14 seconds (20.39 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.05598840190431298

Epoch 45
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00001903
Fine-tune learning rate: 0.00000264
Teacher forcing probability: 0.20802034
Gumbel-softmax tau: 0.15992442
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.0458237289321588
Time for this training epoch: 1229.84 seconds (20.50 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.05591777820231469

Epoch 46
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00001697
Fine-tune learning rate: 0.00000245
Teacher forcing probability: 0.19002034
Gumbel-softmax tau: 0.14788884
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.04383638249165461
Time for this training epoch: 1222.71 seconds (20.38 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.05495838960632682

Epoch 47
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00001502
Fine-tune learning rate: 0.00000227
Teacher forcing probability: 0.17202034
Gumbel-softmax tau: 0.13675902
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.042144083547426686
Time for this training epoch: 1218.39 seconds (20.31 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.05668587765610739

Epoch 48
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00001317
Fine-tune learning rate: 0.00000211
Teacher forcing probability: 0.15402034
Gumbel-softmax tau: 0.12646682
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.04071157206258836
Time for this training epoch: 1223.43 seconds (20.39 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.057195904597924

Epoch 49
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00001144
Fine-tune learning rate: 0.00000195
Teacher forcing probability: 0.13602034
Gumbel-softmax tau: 0.11694918
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.039869120931892314
Time for this training epoch: 1217.64 seconds (20.29 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.05653921342385349

Epoch 50
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00000982
Fine-tune learning rate: 0.00000180
Teacher forcing probability: 0.11802034
Gumbel-softmax tau: 0.10814783
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.03944877484722806
Time for this training epoch: 1222.10 seconds (20.37 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.05658878112382599
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to tf_omr_train/checkpoints/epoch_50_checkpoint.pth
Saving training stats csv

Epoch 51
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00000833
Fine-tune learning rate: 0.00000167
Teacher forcing probability: 0.10002034
Gumbel-softmax tau: 0.10000884
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.037696887724923654
Time for this training epoch: 1217.87 seconds (20.30 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.05763054524621825

Epoch 52
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00000697
Fine-tune learning rate: 0.00000154
Teacher forcing probability: 0.10000000
Gumbel-softmax tau: 0.10000000
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.03685302313392986
Time for this training epoch: 1220.03 seconds (20.33 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.05617632753928619

Epoch 53
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00000573
Fine-tune learning rate: 0.00000143
Teacher forcing probability: 0.10000000
Gumbel-softmax tau: 0.10000000
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.03539435098343246
Time for this training epoch: 1218.54 seconds (20.31 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.057018797609335516

Epoch 54
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00000464
Fine-tune learning rate: 0.00000133
Teacher forcing probability: 0.10000000
Gumbel-softmax tau: 0.10000000
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.034349210032752174
Time for this training epoch: 1222.18 seconds (20.37 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.056513517211352206

Epoch 55
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00000368
Fine-tune learning rate: 0.00000124
Teacher forcing probability: 0.10000000
Gumbel-softmax tau: 0.10000000
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.03363191007456749
Time for this training epoch: 1223.84 seconds (20.40 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.05621181691744562

Epoch 56
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00000287
Fine-tune learning rate: 0.00000117
Teacher forcing probability: 0.10000000
Gumbel-softmax tau: 0.10000000
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.03273841627904007
Time for this training epoch: 1217.70 seconds (20.29 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.0569137587998388

Epoch 57
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00000220
Fine-tune learning rate: 0.00000111
Teacher forcing probability: 0.10000000
Gumbel-softmax tau: 0.10000000
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.03201232000078304
Time for this training epoch: 1219.72 seconds (20.33 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.056035577354313276

Epoch 58
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00000168
Fine-tune learning rate: 0.00000106
Teacher forcing probability: 0.10000000
Gumbel-softmax tau: 0.10000000
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.03157424834428192
Time for this training epoch: 1223.06 seconds (20.38 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.056507831182219644

Epoch 59
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00000130
Fine-tune learning rate: 0.00000103
Teacher forcing probability: 0.10000000
Gumbel-softmax tau: 0.10000000
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.03135436576936898
Time for this training epoch: 1223.71 seconds (20.40 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.05641126655874206

Epoch 60
--------------------------------------------------
Hyperparameters at epoch start:
Base learning rate: 0.00000108
Fine-tune learning rate: 0.00000101
Teacher forcing probability: 0.10000000
Gumbel-softmax tau: 0.10000000
Using hard sampling: True
Starting training
[   16/56612]
[ 1616/56612]
[ 3216/56612]
[ 4816/56612]
[ 6416/56612]
[ 8016/56612]
[ 9616/56612]
[11216/56612]
[12816/56612]
[14416/56612]
[16016/56612]
[17616/56612]
[19216/56612]
[20816/56612]
[22416/56612]
[24016/56612]
[25616/56612]
[27216/56612]
[28816/56612]
[30416/56612]
[32016/56612]
[33616/56612]
[35216/56612]
[36816/56612]
[38416/56612]
[40016/56612]
[41616/56612]
[43216/56612]
[44816/56612]
[46416/56612]
[48016/56612]
[49616/56612]
[51216/56612]
[52816/56612]
[54416/56612]
[56016/56612]
Average training loss over this epoch: 0.031175850359043362
Time for this training epoch: 1224.13 seconds (20.40 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.056125527933867436
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to tf_omr_train/checkpoints/epoch_60_checkpoint.pth
Saving training stats csv
Saving final omr training state
Saving omr training state to tf_omr_train/ending_omr_train_state.pth
Saving final model state dict separately to tf_omr_train/vitomr.pth
