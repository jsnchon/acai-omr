Setting up MAE with mask ratio 0.75 and patch size 16
Using device cuda
Setting up encoder with patch size 16, pe grid of 60 x 200, fine-tuning last 12 layers
Setting up decoder with max lmx sequence length 1536, vocab file lmx_vocab.txt
Loaded pretrained mae state dict from mae_pre_train/pretrained_mae.pth
Setting up ViTOMR model
Model architecture
--------------------
ViTOMR(
  (encoder): FineTuneOMREncoder(
    (projection): Linear(in_features=256, out_features=768, bias=True)
    (fine_tune_blocks): TransformerEncoder(
      (layers): ModuleList(
        (0-11): 12 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (linear1): Linear(in_features=768, out_features=3072, bias=True)
          (dropout): Dropout(p=0.05, inplace=False)
          (linear2): Linear(in_features=3072, out_features=768, bias=True)
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.05, inplace=False)
          (dropout2): Dropout(p=0.05, inplace=False)
        )
      )
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (decoder): OMRDecoder(
    (vocab_embedding): Embedding(227, 1024, padding_idx=1)
    (decoder_blocks): TransformerDecoder(
      (layers): ModuleList(
        (0-23): 24 x TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=4096, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (unembed): Linear(in_features=1024, out_features=227, bias=True)
  )
  (transition_head): Sequential(
    (0): Linear(in_features=768, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.05, inplace=False)
    (3): Linear(in_features=4096, out_features=1024, bias=True)
  )
)
Trainable parameters count
--------------------
Encoder: 94469376
Transition head: 7345152
Decoder: 405160163
Total: 506974691
Setting up DataLoaders with batch size 16, shuffle, 26 workers, ragged collate function, pinned memory
Dataset augmentation probability: 0.25
Creating optimizer parameter groups using base lr 0.0001 for transition head and decoder, 1e-05 as encoder fine-tune base lr with 0.9 layer-wise decay factor
Encoder fine-tune base lrs by layer: [1e-05, 9e-06, 8.1e-06, 7.290000000000001e-06, 6.561e-06, 5.904900000000001e-06, 5.314410000000001e-06, 4.782969000000001e-06, 4.304672100000001e-06, 3.8742048900000015e-06, 3.486784401000001e-06, 3.138105960900001e-06]
Setting up AdamW with betas (0.9, 0.95), weight decay 0.0002
Accumulating gradients for 4 steps for an effective batch size of 64
Setting up scheduler with 2 warm-up epochs, 20 total epochs, 1e-06 minimum learning rate, 885 (effective) batches per epoch
Using label smoothing of 0.0 for cross entropy loss
Created directories omr_train, omr_train/checkpoints, omr_train/stats
OMR training for 20 epochs. Checkpointing every 2 epochs
Epoch 1
--------------------
Base learning rate at epoch start: 0.00000050
Fine-tune learning rate at epoch start: 0.00000005
Starting training
/home/ubuntu/.cache/pypoetry/virtualenvs/acai-omr-BQvANxl_-py3.12/lib/python3.12/site-packages/torch/nested/__init__.py:117: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)
  return torch._nested_tensor_from_tensor_list(ts, dtype, None, device, None)
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 2.153473716076568
Time for this training epoch: 1428.32 seconds (23.81 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 1.108013547559791
Epoch 2
--------------------
Base learning rate at epoch start: 0.00005025
Fine-tune learning rate at epoch start: 0.00000503
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
/home/ubuntu/.cache/pypoetry/virtualenvs/acai-omr-BQvANxl_-py3.12/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Average training loss over this epoch: 0.8805531998177847
Time for this training epoch: 1427.40 seconds (23.79 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.6535131706993209
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to omr_train/checkpoints/epoch_2_checkpoint.pth
Checkpointing stats plots
Saving plot to omr_train/stats/losses.png
Saving plot to omr_train/stats/lrs.png
Writing training stats csv to omr_train/stats/training_stats.csv
Epoch 3
--------------------
Base learning rate at epoch start: 0.00010000
Fine-tune learning rate at epoch start: 0.00001000
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.5285193287204571
Time for this training epoch: 1412.00 seconds (23.53 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.40636080261995033
Epoch 4
--------------------
Base learning rate at epoch start: 0.00009925
Fine-tune learning rate at epoch start: 0.00000993
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.35258244495079927
Time for this training epoch: 1391.81 seconds (23.20 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.2661260337844841
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to omr_train/checkpoints/epoch_4_checkpoint.pth
Checkpointing stats plots
Saving plot to omr_train/stats/losses.png
Saving plot to omr_train/stats/lrs.png
Writing training stats csv to omr_train/stats/training_stats.csv
Epoch 5
--------------------
Base learning rate at epoch start: 0.00009701
Fine-tune learning rate at epoch start: 0.00000973
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.2534614728016852
Time for this training epoch: 1395.85 seconds (23.26 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.20366084755166
Epoch 6
--------------------
Base learning rate at epoch start: 0.00009337
Fine-tune learning rate at epoch start: 0.00000940
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.19019363663920105
Time for this training epoch: 1404.03 seconds (23.40 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.15637649588588712
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to omr_train/checkpoints/epoch_6_checkpoint.pth
Checkpointing stats plots
Saving plot to omr_train/stats/losses.png
Saving plot to omr_train/stats/lrs.png
Writing training stats csv to omr_train/stats/training_stats.csv
Epoch 7
--------------------
Base learning rate at epoch start: 0.00008842
Fine-tune learning rate at epoch start: 0.00000895
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.14935163971890364
Time for this training epoch: 1392.25 seconds (23.20 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.12950947959381126
Epoch 8
--------------------
Base learning rate at epoch start: 0.00008232
Fine-tune learning rate at epoch start: 0.00000839
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.11958714127519536
Time for this training epoch: 1405.01 seconds (23.42 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.11649835754686327
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to omr_train/checkpoints/epoch_8_checkpoint.pth
Checkpointing stats plots
Saving plot to omr_train/stats/losses.png
Saving plot to omr_train/stats/lrs.png
Writing training stats csv to omr_train/stats/training_stats.csv
Epoch 9
--------------------
Base learning rate at epoch start: 0.00007525
Fine-tune learning rate at epoch start: 0.00000775
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.09760766851658638
Time for this training epoch: 1396.87 seconds (23.28 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.09995087665884989
Epoch 10
--------------------
Base learning rate at epoch start: 0.00006743
Fine-tune learning rate at epoch start: 0.00000704
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.08035891875940449
Time for this training epoch: 1390.55 seconds (23.18 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.09207397832005008
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to omr_train/checkpoints/epoch_10_checkpoint.pth
Checkpointing stats plots
Saving plot to omr_train/stats/losses.png
Saving plot to omr_train/stats/lrs.png
Writing training stats csv to omr_train/stats/training_stats.csv
Epoch 11
--------------------
Base learning rate at epoch start: 0.00005910
Fine-tune learning rate at epoch start: 0.00000628
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.06703576667475883
Time for this training epoch: 1369.88 seconds (22.83 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.0888362630391553
Epoch 12
--------------------
Base learning rate at epoch start: 0.00005050
Fine-tune learning rate at epoch start: 0.00000550
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.055626111828510745
Time for this training epoch: 1371.81 seconds (22.86 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.07902422838850316
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to omr_train/checkpoints/epoch_12_checkpoint.pth
Checkpointing stats plots
Saving plot to omr_train/stats/losses.png
Saving plot to omr_train/stats/lrs.png
Writing training stats csv to omr_train/stats/training_stats.csv
Epoch 13
--------------------
Base learning rate at epoch start: 0.00004190
Fine-tune learning rate at epoch start: 0.00000472
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.04635457675161925
Time for this training epoch: 1370.22 seconds (22.84 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.07738047913272879
Epoch 14
--------------------
Base learning rate at epoch start: 0.00003357
Fine-tune learning rate at epoch start: 0.00000396
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.03884553676379008
Time for this training epoch: 1372.99 seconds (22.88 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.07550887736890997
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to omr_train/checkpoints/epoch_14_checkpoint.pth
Checkpointing stats plots
Saving plot to omr_train/stats/losses.png
Saving plot to omr_train/stats/lrs.png
Writing training stats csv to omr_train/stats/training_stats.csv
Epoch 15
--------------------
Base learning rate at epoch start: 0.00002575
Fine-tune learning rate at epoch start: 0.00000325
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.032625824573556894
Time for this training epoch: 1362.53 seconds (22.71 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.07508310834382738
Epoch 16
--------------------
Base learning rate at epoch start: 0.00001868
Fine-tune learning rate at epoch start: 0.00000261
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.02796510344844361
Time for this training epoch: 1358.29 seconds (22.64 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.0735682522508699
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to omr_train/checkpoints/epoch_16_checkpoint.pth
Checkpointing stats plots
Saving plot to omr_train/stats/losses.png
Saving plot to omr_train/stats/lrs.png
Writing training stats csv to omr_train/stats/training_stats.csv
Epoch 17
--------------------
Base learning rate at epoch start: 0.00001258
Fine-tune learning rate at epoch start: 0.00000205
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.024235691620743807
Time for this training epoch: 1351.23 seconds (22.52 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.07359120583554893
Epoch 18
--------------------
Base learning rate at epoch start: 0.00000763
Fine-tune learning rate at epoch start: 0.00000160
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.021723187050520825
Time for this training epoch: 1358.90 seconds (22.65 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.07380841995702624
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to omr_train/checkpoints/epoch_18_checkpoint.pth
Checkpointing stats plots
Saving plot to omr_train/stats/losses.png
Saving plot to omr_train/stats/lrs.png
Writing training stats csv to omr_train/stats/training_stats.csv
Epoch 19
--------------------
Base learning rate at epoch start: 0.00000399
Fine-tune learning rate at epoch start: 0.00000127
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.019956754625996765
Time for this training epoch: 1349.06 seconds (22.48 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.07325046133599493
Epoch 20
--------------------
Base learning rate at epoch start: 0.00000175
Fine-tune learning rate at epoch start: 0.00000107
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.019144631338293402
Time for this training epoch: 1351.13 seconds (22.52 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.07341699181064995
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to omr_train/checkpoints/epoch_20_checkpoint.pth
Checkpointing stats plots
Saving plot to omr_train/stats/losses.png
Saving plot to omr_train/stats/lrs.png
Writing training stats csv to omr_train/stats/training_stats.csv
Plotting final stats
Saving plot to omr_train/stats/losses.png
Saving plot to omr_train/stats/lrs.png
Writing training stats csv to omr_train/stats/training_stats.csv
Saving final omr training state
Saving omr training state to omr_train/ending_omr_train_state.pth
Saving final model state dict separately to omr_train/vitomr.pth
