Setting up MAE with mask ratio 0.75 and patch size 16
Using device cuda
Setting up encoder with patch size 16, pe grid of 60 x 200, fine-tuning last 12 layers
Setting up decoder with max lmx sequence length 1536, vocab file lmx_vocab.txt
Loaded pretrained mae state dict from mae_pre_train/pretrained_mae.pth
Setting up ViTOMR model
Model architecture
--------------------
ViTOMR(
  (encoder): FineTuneOMREncoder(
    (projection): Linear(in_features=256, out_features=768, bias=True)
    (fine_tune_blocks): TransformerEncoder(
      (layers): ModuleList(
        (0-11): 12 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (linear1): Linear(in_features=768, out_features=3072, bias=True)
          (dropout): Dropout(p=0.05, inplace=False)
          (linear2): Linear(in_features=3072, out_features=768, bias=True)
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.05, inplace=False)
          (dropout2): Dropout(p=0.05, inplace=False)
        )
      )
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (decoder): OMRDecoder(
    (vocab_embedding): Embedding(227, 1024, padding_idx=1)
    (decoder_blocks): TransformerDecoder(
      (layers): ModuleList(
        (0-23): 24 x TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=4096, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (unembed): Linear(in_features=1024, out_features=227, bias=True)
  )
  (transition_head): Sequential(
    (0): Linear(in_features=768, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.05, inplace=False)
    (3): Linear(in_features=4096, out_features=1024, bias=True)
  )
)
Trainable parameters count
--------------------
Encoder: 94469376
Transition head: 7345152
Decoder: 405160163
Total: 506974691
Setting up DataLoaders with batch size 16, shuffle, 26 workers, ragged collate function, pinned memory
Dataset augmentation probability: 0.3
Creating optimizer parameter groups using base lr 0.0001 for transition head and decoder, 1e-05 as encoder fine-tune base lr with 0.9 layer-wise decay factor
Encoder fine-tune base lrs by layer: [1e-05, 9e-06, 8.1e-06, 7.290000000000001e-06, 6.561e-06, 5.904900000000001e-06, 5.314410000000001e-06, 4.782969000000001e-06, 4.304672100000001e-06, 3.8742048900000015e-06, 3.486784401000001e-06, 3.138105960900001e-06]
Setting up AdamW with betas (0.9, 0.95), weight decay 0.001
Accumulating gradients for 4 steps for an effective batch size of 64
Setting up scheduler with 2 warm-up epochs, 20 total epochs, 1e-06 minimum learning rate, 885 (effective) batches per epoch
Using label smoothing of 0.0 for cross entropy loss
Created directories omr_train, omr_train/checkpoints, omr_train/stats
OMR training for 20 epochs. Checkpointing every 5 epochs
Epoch 1
--------------------
Base learning rate at epoch start: 0.00000050
Fine-tune learning rate at epoch start: 0.00000005
Starting training
/home/ubuntu/.cache/pypoetry/virtualenvs/acai-omr-BQvANxl_-py3.12/lib/python3.12/site-packages/torch/nested/__init__.py:117: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)
  return torch._nested_tensor_from_tensor_list(ts, dtype, None, device, None)
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 2.160780403818846
Time for this training epoch: 1118.53 seconds (18.64 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 1.1144873844280934
Epoch 2
--------------------
Base learning rate at epoch start: 0.00005025
Fine-tune learning rate at epoch start: 0.00000503
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
/home/ubuntu/.cache/pypoetry/virtualenvs/acai-omr-BQvANxl_-py3.12/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Average training loss over this epoch: 0.8885706754416589
Time for this training epoch: 1113.07 seconds (18.55 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.6719679277715906
Epoch 3
--------------------
Base learning rate at epoch start: 0.00010000
Fine-tune learning rate at epoch start: 0.00001000
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.5303503582625108
Time for this training epoch: 1114.19 seconds (18.57 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.40775746799735374
Epoch 4
--------------------
Base learning rate at epoch start: 0.00009925
Fine-tune learning rate at epoch start: 0.00000993
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.35217065029593636
Time for this training epoch: 1172.54 seconds (19.54 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.27126536491328973
Epoch 5
--------------------
Base learning rate at epoch start: 0.00009701
Fine-tune learning rate at epoch start: 0.00000973
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.25191416369298986
Time for this training epoch: 1181.65 seconds (19.69 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.20011529513894877
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to omr_train/checkpoints/epoch_5_checkpoint.pth
Checkpointing stats plots
Saving plot to omr_train/stats/losses.png
Saving plot to omr_train/stats/lrs.png
Writing training stats csv to omr_train/stats/training_stats.csv
Epoch 6
--------------------
Base learning rate at epoch start: 0.00009337
Fine-tune learning rate at epoch start: 0.00000940
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.18918902008007663
Time for this training epoch: 1185.35 seconds (19.76 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.15572431418242486
Epoch 7
--------------------
Base learning rate at epoch start: 0.00008842
Fine-tune learning rate at epoch start: 0.00000895
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.14794468127273172
Time for this training epoch: 1177.10 seconds (19.62 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.12626937221584797
Epoch 8
--------------------
Base learning rate at epoch start: 0.00008232
Fine-tune learning rate at epoch start: 0.00000839
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.11992709125926616
Time for this training epoch: 1182.40 seconds (19.71 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.11002258841258122
Epoch 9
--------------------
Base learning rate at epoch start: 0.00007525
Fine-tune learning rate at epoch start: 0.00000775
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.09680685740807411
Time for this training epoch: 1177.04 seconds (19.62 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.10645109249044583
Epoch 10
--------------------
Base learning rate at epoch start: 0.00006743
Fine-tune learning rate at epoch start: 0.00000704
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.07973528370381193
Time for this training epoch: 1175.67 seconds (19.59 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.09064056459012063
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to omr_train/checkpoints/epoch_10_checkpoint.pth
Checkpointing stats plots
Saving plot to omr_train/stats/losses.png
Saving plot to omr_train/stats/lrs.png
Writing training stats csv to omr_train/stats/training_stats.csv
Epoch 11
--------------------
Base learning rate at epoch start: 0.00005910
Fine-tune learning rate at epoch start: 0.00000628
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.06652212247418782
Time for this training epoch: 1177.46 seconds (19.62 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.08496004071380538
Epoch 12
--------------------
Base learning rate at epoch start: 0.00005050
Fine-tune learning rate at epoch start: 0.00000550
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.05541043642747281
Time for this training epoch: 1178.56 seconds (19.64 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.07840265383693709
Epoch 13
--------------------
Base learning rate at epoch start: 0.00004190
Fine-tune learning rate at epoch start: 0.00000472
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.04617619010698504
Time for this training epoch: 1180.74 seconds (19.68 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.07622277507308259
Epoch 14
--------------------
Base learning rate at epoch start: 0.00003357
Fine-tune learning rate at epoch start: 0.00000396
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.03869224033747674
Time for this training epoch: 1173.47 seconds (19.56 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.07349053549486945
Epoch 15
--------------------
Base learning rate at epoch start: 0.00002575
Fine-tune learning rate at epoch start: 0.00000325
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.03272961064245248
Time for this training epoch: 1177.38 seconds (19.62 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.07262840755045541
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to omr_train/checkpoints/epoch_15_checkpoint.pth
Checkpointing stats plots
Saving plot to omr_train/stats/losses.png
Saving plot to omr_train/stats/lrs.png
Writing training stats csv to omr_train/stats/training_stats.csv
Epoch 16
--------------------
Base learning rate at epoch start: 0.00001868
Fine-tune learning rate at epoch start: 0.00000261
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.02804236143577963
Time for this training epoch: 1177.28 seconds (19.62 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.07299675490421209
Epoch 17
--------------------
Base learning rate at epoch start: 0.00001258
Fine-tune learning rate at epoch start: 0.00000205
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.024540723940422303
Time for this training epoch: 1175.28 seconds (19.59 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.07296940436495393
Epoch 18
--------------------
Base learning rate at epoch start: 0.00000763
Fine-tune learning rate at epoch start: 0.00000160
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.02195806084945482
Time for this training epoch: 1181.94 seconds (19.70 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.07143930236159611
Epoch 19
--------------------
Base learning rate at epoch start: 0.00000399
Fine-tune learning rate at epoch start: 0.00000127
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.020362118898024327
Time for this training epoch: 1177.27 seconds (19.62 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.07150380169627255
Epoch 20
--------------------
Base learning rate at epoch start: 0.00000175
Fine-tune learning rate at epoch start: 0.00000107
Starting training
[    16/ 56612]
[  1616/ 56612]
[  3216/ 56612]
[  4816/ 56612]
[  6416/ 56612]
[  8016/ 56612]
[  9616/ 56612]
[ 11216/ 56612]
[ 12816/ 56612]
[ 14416/ 56612]
[ 16016/ 56612]
[ 17616/ 56612]
[ 19216/ 56612]
[ 20816/ 56612]
[ 22416/ 56612]
[ 24016/ 56612]
[ 25616/ 56612]
[ 27216/ 56612]
[ 28816/ 56612]
[ 30416/ 56612]
[ 32016/ 56612]
[ 33616/ 56612]
[ 35216/ 56612]
[ 36816/ 56612]
[ 38416/ 56612]
[ 40016/ 56612]
[ 41616/ 56612]
[ 43216/ 56612]
[ 44816/ 56612]
[ 46416/ 56612]
[ 48016/ 56612]
[ 49616/ 56612]
[ 51216/ 56612]
[ 52816/ 56612]
[ 54416/ 56612]
[ 56016/ 56612]
Average training loss over this epoch: 0.0194479135286424
Time for this training epoch: 1178.52 seconds (19.64 minutes)
Starting validation
[   16/ 7499]
[  816/ 7499]
[ 1616/ 7499]
[ 2416/ 7499]
[ 3216/ 7499]
[ 4016/ 7499]
[ 4816/ 7499]
[ 5616/ 7499]
[ 6416/ 7499]
[ 7216/ 7499]
Average validation loss for this epoch: 0.07190635188945385
Checkpointing model, optimizer, scheduler state dicts
Saving omr training state to omr_train/checkpoints/epoch_20_checkpoint.pth
Checkpointing stats plots
Saving plot to omr_train/stats/losses.png
Saving plot to omr_train/stats/lrs.png
Writing training stats csv to omr_train/stats/training_stats.csv
Plotting final stats
Saving plot to omr_train/stats/losses.png
Saving plot to omr_train/stats/lrs.png
Writing training stats csv to omr_train/stats/training_stats.csv
Saving final omr training state
Saving omr training state to omr_train/ending_omr_train_state.pth
Saving final model state dict separately to omr_train/vitomr.pth
