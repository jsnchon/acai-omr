Setting up MAE with mask ratio 0.75 and patch size 16
Using device cuda
Setting up encoder with patch size 16, pe grid of 60 x 200, fine-tuning last 12 layers
Setting up decoder with max lmx sequence length 1536, vocab file lmx_vocab.txt
Loaded pretrained mae state dict from mae_pre_train/pretrained_mae.pth
Setting up ViTOMR model
Loading state dict from omr_train/vitomr.pth
Using device cuda
Creating ViTOMR model from loaded state dict
Model architecture
--------------------
ViTOMR(
  (encoder): FineTuneOMREncoder(
    (projection): Linear(in_features=256, out_features=768, bias=True)
    (fine_tune_blocks): TransformerEncoder(
      (layers): ModuleList(
        (0-11): 12 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (linear1): Linear(in_features=768, out_features=3072, bias=True)
          (dropout): Dropout(p=0.05, inplace=False)
          (linear2): Linear(in_features=3072, out_features=768, bias=True)
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.05, inplace=False)
          (dropout2): Dropout(p=0.05, inplace=False)
        )
      )
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (decoder): OMRDecoder(
    (vocab_embedding): Embedding(227, 1024, padding_idx=1)
    (decoder_blocks): TransformerDecoder(
      (layers): ModuleList(
        (0-23): 24 x TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (linear1): Linear(in_features=1024, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=4096, out_features=1024, bias=True)
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (unembed): Linear(in_features=1024, out_features=227, bias=True)
  )
  (transition_head): Sequential(
    (0): Linear(in_features=768, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.05, inplace=False)
    (3): Linear(in_features=4096, out_features=1024, bias=True)
  )
)
Setting up test dataset and dataloader
Using a batch size of 16 and 26 workers
Starting evaluation
/home/ubuntu/.cache/pypoetry/virtualenvs/acai-omr-BQvANxl_-py3.12/lib/python3.12/site-packages/torch/nested/__init__.py:117: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)
  return torch._nested_tensor_from_tensor_list(ts, dtype, None, device, None)
[   16/10647]
[  416/10647]
[  816/10647]
[ 1216/10647]
[ 1616/10647]
[ 2016/10647]
[ 2416/10647]
[ 2816/10647]
[ 3216/10647]
[ 3616/10647]
[ 4016/10647]
[ 4416/10647]
[ 4816/10647]
[ 5216/10647]
[ 5616/10647]
[ 6016/10647]
[ 6416/10647]
[ 6816/10647]
[ 7216/10647]
[ 7616/10647]
[ 8016/10647]
[ 8416/10647]
[ 8816/10647]
[ 9216/10647]
[ 9616/10647]
[10016/10647]
[10416/10647]
Average test loss: 0.08103396801568859
Creating directory at vitomr_predictions if it already doesn't exist
Saving input image to vitomr_predictions/sample_0/input_image.png
Saving predicted token sequence to vitomr_predictions/sample_0/pred.txt
Saving target token sequence to vitomr_predictions/sample_0/target_seq.txt
Saving input image to vitomr_predictions/sample_1/input_image.png
Saving predicted token sequence to vitomr_predictions/sample_1/pred.txt
Saving target token sequence to vitomr_predictions/sample_1/target_seq.txt
Saving input image to vitomr_predictions/sample_2/input_image.png
Saving predicted token sequence to vitomr_predictions/sample_2/pred.txt
Saving target token sequence to vitomr_predictions/sample_2/target_seq.txt
Saving input image to vitomr_predictions/sample_3/input_image.png
Saving predicted token sequence to vitomr_predictions/sample_3/pred.txt
Saving target token sequence to vitomr_predictions/sample_3/target_seq.txt
Saving input image to vitomr_predictions/sample_4/input_image.png
Saving predicted token sequence to vitomr_predictions/sample_4/pred.txt
Saving target token sequence to vitomr_predictions/sample_4/target_seq.txt
